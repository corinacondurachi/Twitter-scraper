#  Athena-2019-Twitter
Twython is an advanced Twitter scraping tool written in Python that allows for scraping Tweets, Followers, Username, Desciption and so on from Twitter profiles using Twitter's API.


## Limits imposed by Twitter

 - Get_followera_list - 15 Requests / 15-min window (user auth)


## Requirements

 - Python 3.6;
 - Pip;
 - Pymongo;
 - Twython;
 - Cvs;
 - Time;
 - Requsts;
 - Flask;

## Installing
**Installing Python3.6**

	sudo apt-get update
	sudo apt-get install python3.6

**Installing Pip for Python3**
		
	sudo apt update
	sudo apt install python3-pip
	
**Installing MongoDB**

	sudo apt update
	sudo apt install mongodb-org
	
**Download  Robo3T** 
In order to view the database created, you need to download 	Robo3T by accessing this link https://robomongo.org/download

**Installing Twython**

	pip install twython
**Installing Requests**

	pip install requests`

**Installing Flask**

	pip install Flask

## Describing the code
First, I create a database called 'twitter_db' which is comprised of 3 collections ('users_data', 'inter' and 'queue'). 'Users_data' is the one in which I stock user's information (id, username, screen_name, description, location, url). 'Inter' is an intermediary collection in which I add users which are about to be extracted in order to collect their information and add it to the 'Users_data' collection. 'Queue' is a collection in which I add users in order to extract their followers. Once I do that, I add them both in 'inter' and in 'queue' collections. Also, 'inter' and 'queue' collections have an unique key, so we cannot add the same user. 

# 1. Scraper 

**Class Twitterminer-Methods**

 - def init_api(self): 
	 sets the consumer_key, consumer_secret, access_token and access_token_secret and uses them in Twython method. These keys are generated by Twitter and can be obtained by creating a Twitter Developer Account.
	 
 - def extract_followers(self,user,followers_no)
	 extracts a user's followers. As Twitter imposes a limit of 15 requests/15 minutes, I decided to make a request per minute. The maximum number of users allowed to download is 200, so I have to divide the number of one's followers into pieces of 200. The users extracted are added in 'queue' and 'inter' collections.
	 
 - def extract_user_data(self, user=" set default user to get data from")
	extracts information about a user and adds it into 'users_data' collection. For every user we add username and screen_name. If location, description or URL are provided, we add them to the database.
	
	
 -    def extract_tweets_by_geolocation(self,latitude, longitude, max_range) :
 extracts every tweet that has been posted in that region. We should give latitude, longitude and max_range (in km or miles) of the place. I extract when the tweet was posted, its id, text and coordinates if there are any provided.
 
 - def extract_locations_of_a_user(self,username) :
 this function extracts the location from the last 200 tweets of a given user. Along with username and it, it also extracts information about the place such as coordinates.
	
 - def extract_users_inter_collection(self):
 extracts users from 'inter' database and adds their information into 'users_data' collection. It also deletes them once they are extracted. 
 
 - def initialize(self):
 extracts data of a given user and its followers. It is called only once, being the starting point of the project.
 
 # 2. Project
 Imports scraper.py and creates an object. The while loop extracts user's information from 'inter' collection, then extracts the followers of the first member of 'queue' collection and deletes it from the collection once it finished. 
  # 3. Server_user
  Creates a HTTP server which operates on port 9500. It extracts information about a given user from the database. If the user does not exist in the database, it extracts the information from Twitter using scraper.py and adds it in the 'user's data' collection.
  
   # 4. Server_geolocation
 Creates a HTTP server which operates on port 9000.  It extracts the last 200 tweets that have been posted in the specified region. It uses the function  extract_tweets_by_geolocation from scraper.py.
   # 5. Server_locations
Creates a HTTP server which operates on port 8000.  It extracts the locations of the last 200 tweets that have been posted by a specified user. It uses the function  extract_locations_of_a_user from scraper.py.

  
   
  

 

	 

